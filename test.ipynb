{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1321, which is longer than the specified 1000\n",
      "Created a chunk of size 1455, which is longer than the specified 1000\n",
      "Created a chunk of size 1833, which is longer than the specified 1000\n",
      "Created a chunk of size 1066, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "aws_dir = \"./eng-feed/src/aws/lib\"\n",
    "lambda_dir = \"./eng-feed/src/aws/backend/lambda\"\n",
    "db_dir = \"./eng-feed/src/aws/backend/db\"\n",
    "pages_dir = \"./eng-feed/src/pages\"\n",
    "components_dir = \"./eng-feed/src/components\"\n",
    "\n",
    "docs = []\n",
    "for directory in [aws_dir, lambda_dir, db_dir, pages_dir, components_dir]:\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for file in filenames:\n",
    "            try:\n",
    "                loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")\n",
    "                docs.extend(loader.load_and_split())\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating embeddings: 100%|██████████| 1/1 [00:11<00:00, 11.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://haffimazhar96/code-index-llm', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      "   text       text      (91, 1)      str     None   \n",
      " metadata     json      (91, 1)      str     None   \n",
      " embedding  embedding  (91, 1536)  float32   None   \n",
      "    id        text      (91, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2decf00c-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decf278-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decf304-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decf372-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decf3d6-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decf444-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decf50c-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decf570-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decf5d4-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decf638-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decf6a6-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decf70a-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decf818-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decf8d6-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decf9bc-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decfa20-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decfb38-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decfc6e-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decfd9a-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decfdfe-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decfec6-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decff2a-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decff84-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2decffe8-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded004c-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded00b0-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0114-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0178-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded01dc-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0240-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded02a4-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0308-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded036c-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded03d0-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0434-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0498-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded04fc-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded056a-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded05ce-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0632-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0696-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded06fa-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0790-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded07f4-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0858-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded08bc-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0920-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0984-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded09e8-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0ab0-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0b14-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0b78-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0be6-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0c4a-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0cae-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0d08-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0d6c-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0dc6-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0f4c-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded0fa6-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded10c8-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded11f4-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1320-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1384-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded13e8-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded151e-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded15d2-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1636-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded17a8-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1866-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1924-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded197e-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1aa0-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1b04-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1b68-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1bcc-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1cf8-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1d84-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1dde-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1e42-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1ea6-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1f0a-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1f6e-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded1fc8-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded202c-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded2090-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded20ea-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded214e-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded21a8-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded2202-7b73-11ee-bd5d-e24bee7314b9',\n",
       " '2ded2266-7b73-11ee-bd5d-e24bee7314b9']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "deeplake_username = \"haffimazhar96\"\n",
    "\n",
    "db = DeepLake(\n",
    "    dataset_path=f\"hub://{deeplake_username}/code-index-llm\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "db.add_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceNotFoundException",
     "evalue": "Dataset at path hub://haffimazhar96/code-llm is scheduled for deletion.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceNotFoundException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/haff/code/code-llm/test.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/haff/code/code-llm/test.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m db \u001b[39m=\u001b[39m DeepLake(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/haff/code/code-llm/test.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     dataset_path\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhub://\u001b[39;49m\u001b[39m{\u001b[39;49;00mdeeplake_username\u001b[39m}\u001b[39;49;00m\u001b[39m/code-llm\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/haff/code/code-llm/test.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     read_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/haff/code/code-llm/test.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     embedding\u001b[39m=\u001b[39;49membeddings,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/haff/code/code-llm/test.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n",
      "File \u001b[0;32m~/code/code-llm/.llm-venv/lib/python3.11/site-packages/langchain/vectorstores/deeplake.py:174\u001b[0m, in \u001b[0;36mDeepLake.__init__\u001b[0;34m(self, dataset_path, token, embedding, embedding_function, read_only, ingestion_batch_size, num_workers, verbose, exec_option, runtime, index_params, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m embedding_function:\n\u001b[1;32m    169\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    170\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing embedding function is deprecated and will be removed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39min the future. Please use embedding instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 174\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorstore \u001b[39m=\u001b[39m DeepLakeVectorStore(\n\u001b[1;32m    175\u001b[0m     path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_path,\n\u001b[1;32m    176\u001b[0m     embedding_function\u001b[39m=\u001b[39;49membedding_function \u001b[39mor\u001b[39;49;00m embedding,\n\u001b[1;32m    177\u001b[0m     read_only\u001b[39m=\u001b[39;49mread_only,\n\u001b[1;32m    178\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    179\u001b[0m     exec_option\u001b[39m=\u001b[39;49mexec_option,\n\u001b[1;32m    180\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    181\u001b[0m     runtime\u001b[39m=\u001b[39;49mruntime,\n\u001b[1;32m    182\u001b[0m     index_params\u001b[39m=\u001b[39;49mindex_params,\n\u001b[1;32m    183\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    184\u001b[0m )\n\u001b[1;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function \u001b[39m=\u001b[39m embedding_function \u001b[39mor\u001b[39;00m embedding\n\u001b[1;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id_tensor_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorstore\u001b[39m.\u001b[39mtensors() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/code/code-llm/.llm-venv/lib/python3.11/site-packages/deeplake/core/vectorstore/vectorstore_factory.py:29\u001b[0m, in \u001b[0;36mvectorstore_factory\u001b[0;34m(path, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m deepmemory_is_available:\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m DeepMemoryVectorStore(\n\u001b[1;32m     27\u001b[0m         path\u001b[39m=\u001b[39mpath, client\u001b[39m=\u001b[39mdm_client, org_id\u001b[39m=\u001b[39mdataset_id, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m     28\u001b[0m     )\n\u001b[0;32m---> 29\u001b[0m \u001b[39mreturn\u001b[39;00m VectorStore(path\u001b[39m=\u001b[39;49mpath, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/code-llm/.llm-venv/lib/python3.11/site-packages/deeplake/core/vectorstore/deeplake_vectorstore.py:169\u001b[0m, in \u001b[0;36mVectorStore.__init__\u001b[0;34m(self, path, tensor_params, embedding_function, read_only, ingestion_batch_size, index_params, num_workers, exec_option, token, overwrite, verbose, runtime, creds, org_id, logger, branch, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreds \u001b[39m=\u001b[39m creds \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_function \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcreate_embedding_function(embedding_function)\n\u001b[0;32m--> 169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m dataset_utils\u001b[39m.\u001b[39;49mcreate_or_load_dataset(\n\u001b[1;32m    170\u001b[0m     tensor_params,\n\u001b[1;32m    171\u001b[0m     path,\n\u001b[1;32m    172\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoken,\n\u001b[1;32m    173\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreds,\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger,\n\u001b[1;32m    175\u001b[0m     read_only,\n\u001b[1;32m    176\u001b[0m     exec_option,\n\u001b[1;32m    177\u001b[0m     embedding_function,\n\u001b[1;32m    178\u001b[0m     overwrite,\n\u001b[1;32m    179\u001b[0m     runtime,\n\u001b[1;32m    180\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49morg_id,\n\u001b[1;32m    181\u001b[0m     branch,\n\u001b[1;32m    182\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    183\u001b[0m )\n\u001b[1;32m    184\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exec_option \u001b[39m=\u001b[39m exec_option\n\u001b[1;32m    185\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m=\u001b[39m verbose\n",
      "File \u001b[0;32m~/code/code-llm/.llm-venv/lib/python3.11/site-packages/deeplake/core/vectorstore/vector_search/dataset/dataset.py:51\u001b[0m, in \u001b[0;36mcreate_or_load_dataset\u001b[0;34m(tensor_params, dataset_path, token, creds, logger, read_only, exec_option, embedding_function, overwrite, runtime, org_id, branch, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     _INDRA_INSTALLED \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     47\u001b[0m utils\u001b[39m.\u001b[39mcheck_indra_installation(\n\u001b[1;32m     48\u001b[0m     exec_option\u001b[39m=\u001b[39mexec_option, indra_installed\u001b[39m=\u001b[39m_INDRA_INSTALLED\n\u001b[1;32m     49\u001b[0m )\n\u001b[0;32m---> 51\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m overwrite \u001b[39mand\u001b[39;00m dataset_exists(dataset_path, token, creds, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs):\n\u001b[1;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m tensor_params \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m tensor_params \u001b[39m!=\u001b[39m DEFAULT_VECTORSTORE_TENSORS:\n\u001b[1;32m     53\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     54\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mVector Store is not empty. You shouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt specify tensor_params if you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre loading from existing dataset.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m         )\n",
      "File \u001b[0;32m~/code/code-llm/.llm-venv/lib/python3.11/site-packages/deeplake/core/vectorstore/vector_search/dataset/dataset.py:86\u001b[0m, in \u001b[0;36mdataset_exists\u001b[0;34m(dataset_path, token, creds, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdataset_exists\u001b[39m(dataset_path, token, creds, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     85\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m---> 86\u001b[0m         deeplake\u001b[39m.\u001b[39;49mexists(dataset_path, token\u001b[39m=\u001b[39;49mtoken, creds\u001b[39m=\u001b[39;49mcreds)\n\u001b[1;32m     87\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n\u001b[1;32m     88\u001b[0m     )\n",
      "File \u001b[0;32m~/code/code-llm/.llm-venv/lib/python3.11/site-packages/deeplake/api/dataset.py:342\u001b[0m, in \u001b[0;36mdataset.exists\u001b[0;34m(path, creds, token)\u001b[0m\n\u001b[1;32m    340\u001b[0m     creds \u001b[39m=\u001b[39m {}\n\u001b[1;32m    341\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 342\u001b[0m     storage, cache_chain \u001b[39m=\u001b[39m get_storage_and_cache_chain(\n\u001b[1;32m    343\u001b[0m         path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m    344\u001b[0m         read_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    345\u001b[0m         creds\u001b[39m=\u001b[39;49mcreds,\n\u001b[1;32m    346\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    347\u001b[0m         memory_cache_size\u001b[39m=\u001b[39;49mDEFAULT_MEMORY_CACHE_SIZE,\n\u001b[1;32m    348\u001b[0m         local_cache_size\u001b[39m=\u001b[39;49mDEFAULT_LOCAL_CACHE_SIZE,\n\u001b[1;32m    349\u001b[0m     )\n\u001b[1;32m    350\u001b[0m \u001b[39mexcept\u001b[39;00m TokenPermissionError:\n\u001b[1;32m    351\u001b[0m     \u001b[39m# Cloud Dataset does not exist\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/code/code-llm/.llm-venv/lib/python3.11/site-packages/deeplake/util/storage.py:223\u001b[0m, in \u001b[0;36mget_storage_and_cache_chain\u001b[0;34m(path, read_only, creds, token, memory_cache_size, local_cache_size, db_engine)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_storage_and_cache_chain\u001b[39m(\n\u001b[1;32m    198\u001b[0m     path,\n\u001b[1;32m    199\u001b[0m     read_only,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     db_engine\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    205\u001b[0m ):\n\u001b[1;32m    206\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    Returns storage provider and cache chain for a given path, according to arguments passed.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39m        A tuple of the storage provider and the storage chain.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     storage \u001b[39m=\u001b[39m storage_provider_from_path(\n\u001b[1;32m    224\u001b[0m         path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m    225\u001b[0m         db_engine\u001b[39m=\u001b[39;49mdb_engine,\n\u001b[1;32m    226\u001b[0m         creds\u001b[39m=\u001b[39;49mcreds,\n\u001b[1;32m    227\u001b[0m         read_only\u001b[39m=\u001b[39;49mread_only,\n\u001b[1;32m    228\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[1;32m    230\u001b[0m     memory_cache_size_bytes \u001b[39m=\u001b[39m memory_cache_size \u001b[39m*\u001b[39m MB\n\u001b[1;32m    231\u001b[0m     local_cache_size_bytes \u001b[39m=\u001b[39m local_cache_size \u001b[39m*\u001b[39m MB\n",
      "File \u001b[0;32m~/code/code-llm/.llm-venv/lib/python3.11/site-packages/deeplake/util/storage.py:57\u001b[0m, in \u001b[0;36mstorage_provider_from_path\u001b[0;34m(path, creds, read_only, token, is_hub_path, db_engine)\u001b[0m\n\u001b[1;32m     55\u001b[0m     creds \u001b[39m=\u001b[39m {}\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m path\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mhub://\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m     storage: StorageProvider \u001b[39m=\u001b[39m storage_provider_from_hub_path(\n\u001b[1;32m     58\u001b[0m         path, read_only, db_engine\u001b[39m=\u001b[39;49mdb_engine, token\u001b[39m=\u001b[39;49mtoken, creds\u001b[39m=\u001b[39;49mcreds\n\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     60\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[39mif\u001b[39;00m path\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39ms3://\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/code/code-llm/.llm-venv/lib/python3.11/site-packages/deeplake/util/storage.py:150\u001b[0m, in \u001b[0;36mstorage_provider_from_hub_path\u001b[0;34m(path, read_only, db_engine, token, creds)\u001b[0m\n\u001b[1;32m    147\u001b[0m client \u001b[39m=\u001b[39m DeepLakeBackendClient(token\u001b[39m=\u001b[39mtoken)\n\u001b[1;32m    149\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m (read_only \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39melse\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m read_only \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 150\u001b[0m url, final_creds, mode, expiration, repo \u001b[39m=\u001b[39m get_dataset_credentials(\n\u001b[1;32m    151\u001b[0m     client, org_id, ds_name, mode, db_engine\n\u001b[1;32m    152\u001b[0m )\n\u001b[1;32m    154\u001b[0m is_local \u001b[39m=\u001b[39m get_path_type(url) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m \u001b[39m# ignore mode returned from backend if underlying storage is local\u001b[39;00m\n",
      "File \u001b[0;32m~/code/code-llm/.llm-venv/lib/python3.11/site-packages/deeplake/util/storage.py:128\u001b[0m, in \u001b[0;36mget_dataset_credentials\u001b[0;34m(client, org_id, ds_name, mode, db_engine)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dataset_credentials\u001b[39m(\n\u001b[1;32m    120\u001b[0m     client: DeepLakeBackendClient,\n\u001b[1;32m    121\u001b[0m     org_id: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m ):\n\u001b[1;32m    126\u001b[0m     \u001b[39m# this will give the proper url (s3, gcs, etc) and corresponding creds, depending on where the dataset is stored.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         url, final_creds, mode, expiration, repo \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mget_dataset_credentials(\n\u001b[1;32m    129\u001b[0m             org_id, ds_name, mode\u001b[39m=\u001b[39;49mmode, db_engine\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39menabled\u001b[39;49m\u001b[39m\"\u001b[39;49m: db_engine}\n\u001b[1;32m    130\u001b[0m         )\n\u001b[1;32m    131\u001b[0m     \u001b[39mexcept\u001b[39;00m AgreementNotAcceptedError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    132\u001b[0m         handle_dataset_agreements(client, e\u001b[39m.\u001b[39magreements, org_id, ds_name)\n",
      "File \u001b[0;32m~/code/code-llm/.llm-venv/lib/python3.11/site-packages/deeplake/client/client.py:246\u001b[0m, in \u001b[0;36mDeepLakeBackendClient.get_dataset_credentials\u001b[0;34m(self, org_id, ds_name, mode, db_engine, no_cache)\u001b[0m\n\u001b[1;32m    244\u001b[0m relative_url \u001b[39m=\u001b[39m GET_DATASET_CREDENTIALS_SUFFIX\u001b[39m.\u001b[39mformat(org_id, ds_name)\n\u001b[1;32m    245\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 246\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    247\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    248\u001b[0m         relative_url,\n\u001b[1;32m    249\u001b[0m         endpoint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendpoint(),\n\u001b[1;32m    250\u001b[0m         params\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    251\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmode\u001b[39;49m\u001b[39m\"\u001b[39;49m: mode,\n\u001b[1;32m    252\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mno_cache\u001b[39;49m\u001b[39m\"\u001b[39;49m: no_cache,\n\u001b[1;32m    253\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mdb_engine\u001b[39;49m\u001b[39m\"\u001b[39;49m: json\u001b[39m.\u001b[39;49mdumps(db_engine),\n\u001b[1;32m    254\u001b[0m         },\n\u001b[1;32m    255\u001b[0m     )\u001b[39m.\u001b[39mjson()\n\u001b[1;32m    256\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    257\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, AuthorizationException):\n",
      "File \u001b[0;32m~/code/code-llm/.llm-venv/lib/python3.11/site-packages/deeplake/client/client.py:163\u001b[0m, in \u001b[0;36mDeepLakeBackendClient.request\u001b[0;34m(self, method, relative_url, endpoint, params, data, files, json, headers, timeout)\u001b[0m\n\u001b[1;32m    161\u001b[0m     status_code \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code\n\u001b[1;32m    162\u001b[0m     tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 163\u001b[0m check_response_status(response)\n\u001b[1;32m    164\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/code/code-llm/.llm-venv/lib/python3.11/site-packages/deeplake/client/utils.py:92\u001b[0m, in \u001b[0;36mcheck_response_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m404\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[39mif\u001b[39;00m message \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m         \u001b[39mraise\u001b[39;00m ResourceNotFoundException(message)\n\u001b[1;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m ResourceNotFoundException\n\u001b[1;32m     94\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m422\u001b[39m:\n",
      "\u001b[0;31mResourceNotFoundException\u001b[0m: Dataset at path hub://haffimazhar96/code-llm is scheduled for deletion."
     ]
    }
   ],
   "source": [
    "db = DeepLake(\n",
    "    dataset_path=f\"hub://{deeplake_username}/code-index-llm\",\n",
    "    read_only=True,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 100\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\")  # switch to 'gpt-4'\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What does this code repo do? \n",
      "\n",
      "**Answer**: This code repository defines a CDK stack called \"DevFeedStack\" that sets up various AWS resources and configurations. The stack includes the following components:\n",
      "\n",
      "1. Creates an SQS queue and an SNS topic.\n",
      "2. Defines a Lambda function named \"FetchAndInsertPosts\" and schedules it to run every day at 8:00 AM.\n",
      "3. Defines a Lambda function named \"NotifyUsers\" and schedules it to run every day at 9:00 AM (or every Friday at 9:00 AM in production environment).\n",
      "4. Grants permissions to the Lambda functions to access the necessary resources, such as RDS instance and SQS queue.\n",
      "5. Creates a Lambda function named \"SendEmail\" for sending emails and subscribes the SNS topic to it.\n",
      "6. Sets up an SQS event source for the \"NotifyUsers\" Lambda function to process messages from the queue.\n",
      "7. Creates an S3 bucket for storing assets and sets a bucket policy to allow public read access to all objects.\n",
      "8. Deploys assets to the S3 bucket.\n",
      "9. Exports the URL of the SQS queue.\n",
      "\n",
      "Additionally, there are classes and functions defined for convenience and reusability, such as the \"GenericLambda\" class for creating Lambda functions with specific configurations, and the \"ApiGateway\" class for setting up an API Gateway with logging enabled.\n",
      "\n",
      "Overall, this code repo is responsible for setting up the infrastructure and configurations required for a development feed application, including fetching and inserting posts, notifying users, sending emails, and managing assets. \n",
      "\n",
      "-> **Question**: Where is this app hosted? \n",
      "\n",
      "**Answer**: The provided code does not specify where the application is hosted. It includes the creation of various AWS resources such as SQS queues, SNS topics, Lambda functions, and an API Gateway. These resources can be used to build and deploy an application, but the code itself does not indicate the specific hosting environment. \n",
      "\n",
      "-> **Question**: How do users get notified when there is a new post? \n",
      "\n",
      "**Answer**: Users are notified when there is a new post through a Lambda function called \"NotifyUsers\". This Lambda function is scheduled to run at 9:00 AM every Friday using a cron expression. \n",
      "\n",
      "The Lambda function is triggered by messages from an SQS queue. When new posts are fetched and inserted into the database by another Lambda function called \"FetchAndInsertPosts\", they are also added as messages to the SQS queue. \n",
      "\n",
      "The \"NotifyUsers\" Lambda function processes these messages from the queue and sends notifications to the users. The notifications can be sent through various means such as email, SMS, or push notifications, depending on the implementation of the Lambda function. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What does this code repo do?\",\n",
    "    \"Where is this app hosted?\",\n",
    "    \"How do users get notified when there is a new post?\",\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: Which class has the code for aws stack? \n",
      "\n",
      "**Answer**: The class that contains the code for the AWS stack is `DevFeedStack`. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_question = [\n",
    "    \"Which class has the code for aws stack?\"\n",
    "]\n",
    "\n",
    "for question in new_question:\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
